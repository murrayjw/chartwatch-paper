---
title: "pre-assessment performance"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document looks at the model performance of the clinician predictions with a comparison against the MARS CHARTwatch model. The CHARTwatch model usually generates predictions 3 times a day, but for a fair comparison of clinician vs model predictions, we only run the CHARTwatch at times when the clincians generated their predictions. 


```{r echo=F, message=FALSE, warning=FALSE}
# load the require libraries ----------------------------------------------

# load the require libraries ----------------------------------------------
library(pROC)
library(tidyverse)

# threshold from model retraining to Aprils
# Where does this threshold come from?
# Not the same as thresholds from here: https://github.com/LKS-CHART/chartwatch/blob/master/R/constants.R#L4
threshold <- .0905

prediction_outcomes <- readr::read_csv('/mnt/research/LKS-CHART/Projects/gim_ews_preassessment_project/data/practitioner_predictions_outcomes.csv')
load("/mnt/research/LKS-CHART/Projects/gim_ews_preassessment_project/data/model_predictions.Rda")

phys_enc <- unique(prediction_outcomes$ENCOUNTER_NUM)
mod_enc <- unique(model_predictions$ENCOUNTER_NUM)

sd <- setdiff(phys_enc, mod_enc)

clinician <- prediction_outcomes %>% 
  filter(!ENCOUNTER_NUM %in% sd)

clinician <- clinician %>% 
  mutate(outcome = ifelse(icu+death+pal > 0, 1, 0))

distinct_outcomes <- prediction_outcomes %>% 
  distinct(ENCOUNTER_NUM, timestamp, OUTCOME, outcome48, OUTCOME_TS)

model <- model_predictions %>% 
  inner_join(distinct_outcomes, by = c("timestamp", "ENCOUNTER_NUM"))


clinician_sum <- clinician %>% 
  group_by(ENCOUNTER_NUM) %>% 
  # What do mp and mp48 stand for?
  summarize(mp = max(ever),
            mp48 = max(next48),
            outcome = max(outcome),
            outcome_48 = max(outcome48)) %>% 
  ungroup()

model <- model %>% 
  left_join(clinician_sum, by = c("ENCOUNTER_NUM"))




```


## Descriptive statistics

There were 169 clincians who generated 3,375 predictions on  961 patient encounters between April 30th and August 28th 2019.  The model generated 2,737 predictions for the 961 patient encounters. 

How were model predictions computed? Is it possible to also get 3,375 model predictions?

Below is a breakdown of the number of predictions by professional role

```{r, echo=F}

clinician %>% 
  count(professional_role) %>% 
  ggplot(aes(professional_role, n)) +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=n), hjust=-0.1, size=3.5)+
  coord_flip() +
  theme_minimal() +
  ggtitle("Number of predictions by professional role")
```


```{r echo = FALSE, outcomes}
prediction_outcomes %>%
  summarize(num_patients = length(unique(MRN)),
            num_predictions = n(),
            num_encounters = length(unique(ENCOUNTER_NUM)),
            num_outcomes_ever = sum(outcome),
            num_outcome_48 = sum(outcome48)) %>%
  knitr::kable(caption = "Prediction-level summary")

prediction_outcomes %>%
  group_by(ENCOUNTER_NUM) %>%
  summarize(outcome = max(outcome),
            pal = max(pal),
            icu = max(icu),
            death = max(death),
            
            outcome48 = max(outcome48)) %>%
  count(outcome, outcome48) %>%
  mutate(pct  = n / sum(n))  %>%
  knitr::kable(caption = "Results grouped by ENCOUNTER_NUM")

prediction_outcomes %>%
  group_by(MRN) %>%
  summarize(outcome = max(outcome),
            pal = max(pal),
            icu = max(icu),
            death = max(death),
            
            outcome48 = max(outcome48)) %>%
  count(outcome, outcome48) %>%
  mutate(pct  = n / sum(n))  %>%
  knitr::kable(caption = "Results grouped by MRN")

# Difference between prediction_outcomes, distinct_outcomes and clinician variables?
distinct_outcomes %>%
  count(OUTCOME, outcome48) %>%
  knitr::kable(caption = "Breakdown of distinct outcomes")
```
In total there were 73 (73/960 = 7.60%) visits who experienced an outcome at some time after a prediction was made. There were 44 visits (44/960 = 4.58%) that experienced an outcome within 48 hours of a prediction. Does it make sense to compute this at prediction-level? Below is a table of outcomes

```{r, echo=F}

ever <- clinician %>% 
  group_by(ENCOUNTER_NUM) %>% 
  summarize(death_ever = max(death),
            icu_ever = max(icu),
            pal_ever = max(pal)) %>% 
  ungroup() %>% 
  summarize(death = sum(death_ever),
            icu = sum(icu_ever),
            pal = sum(pal_ever))

outcome48 <- clinician %>% 
  filter(outcome48 == 1) %>% 
  group_by(ENCOUNTER_NUM) %>% 
  summarize(death_ever = max(OUTCOME_TYPE == 2),
            icu_ever = max(OUTCOME_TYPE == 1),
            pal_ever = max(OUTCOME_TYPE == 3)) %>% 
  ungroup() %>% 
  summarize(death = sum(death_ever),
            icu = sum(icu_ever),
            pal = sum(pal_ever))

outcome_table <- tibble(`Outcome Type` = c("Death",
                                           "ICU",
                                           "Palliative Transfer"),
                        `Outcome Ever` = c(ever$death, ever$icu, ever$pal),
                        `Outcome in 48 hours` = c(outcome48$death, outcome48$icu,
                                                  outcome48$pal))

knitr::kable(outcome_table,
             caption = "Outcomes of predictions grouped by encounter")

```







## Performance of All Clincians

As a baseline check of model performance, we look at the accuracy of all clinician predictions against the outcome of death/icu/palliative transfer at any point in the patient stay. We can think of each clinician as its own model. This will mean multiple clinicians can predict on the same patient at the same time. Later we will look at average performance per clinician (and by role).  This treats each clinician prediction as a separate event


```{r, warning=F, message=F}
library(caret)

cat("AUC of outcome at any time\n")
roc_obj_ever <- roc(clinician$outcome, clinician$ever, plot=F, ci=TRUE, main = "Outcome at any time")

roc_obj_ever

cat("AUC of outcome in next 48 hours\n")
roc_obj_48 <- roc(clinician$outcome48, clinician$next48, plot=F, ci=TRUE, main = "Outcome in next 48 hours")

roc_obj_48


confusionMatrix(factor(clinician$ever, levels = c(0, 1)), factor(clinician$outcome, levels = c(0,1)), positive = "1")

confusionMatrix(factor(clinician$next48, levels = c(0, 1)), factor(clinician$outcome48, levels = c(0,1)), positive = "1")
```

The AUC of all clinicians together is 0.5962 (95% CI: 0.5743-0.6182) for an outcome occuring at any time and 0.5963 (95% CI: 0.5437-0.6488) in the next 48 hours. The Sensitivity is 0.30 while the positive predictive value is .33. Where are sensitivity and PPV computed?


## Overall performance of the model

AUC of model (ever): 0.7004 (95% CI: 0.6708-0.7299)
AUC of model (next 48 hours): 0.7525 (95% CI: 0.677-0.8279)

```{r, echo=F, message=F, warning=F}

roc_obj_ever <- roc(model$outcome, model$last_score, plot=TRUE, ci=TRUE, main = "Outcome at any time")

roc_obj_ever


# Why is comparison between last_score and outcome columns?
roc_obj_48<- roc(model$outcome48, model$last_score, plot=TRUE, ci=TRUE, main = "Outcome in next 48 hours")

roc_obj_48




confusionMatrix(factor(as.numeric(model$last_score > threshold), levels = c(0, 1)), factor(model$outcome, levels = c(0,1)), positive = "1")

confusionMatrix(factor(as.numeric(model$last_score > threshold), levels = c(0, 1)), factor(model$outcome48, levels = c(0,1)), positive = "1")


```





## Performance per clinician

In what follows we calculate performance metrics for each clinician and then examine those distributions to understand the average clinician performance. There are 169 clincians who provided at least 1 prediction with a median of 7 predictions per clinician (Interquartile Range: 4-16). Clincians provided predictions for a median of 7 patient encounters with an interquartile range of 4-15. 

THere were 54 clinicians (31%) who's patient never experienced an outcome. Of the remaining 115 clinicians, 63 (54.8%) did not correctly identify any outcomes (sensitivity). The plot below displays a histogram of sensitivities for each clinician (median = 0.0, IQR: 0.0 -.50). Eighteen clinicians had perfect sensitivity. 


```{r, echo=F}
clinician %>%
  summarize(
    num_clinicians = length(unique(clinicianid)),
    num_predictions = n()
  )  %>%
  mutate(preds_per_clinician = num_predictions / num_clinicians) %>%
  knitr::kable(caption = "Clinician summary")

clinician_sum_ever <- clinician %>% 
  group_by(clinicianid) %>% 
  summarize(role = max(professional_role),
            number_predictions = n(),
            number_positive = sum(ever),
            number_negative = sum(ever == 0),
            number_encounters = n_distinct(ENCOUNTER_NUM),
            patients_with_outcome = n_distinct(ENCOUNTER_NUM[outcome == 1]),
            correct_positive = sum(ever ==1 & outcome == 1),
            correct_negative = sum(ever == 0 & outcome == 0),
            false_negative = sum(ever == 0 & outcome == 1),
            false_positive = sum(ever == 1 & outcome == 0),
            patients_identified = n_distinct(ENCOUNTER_NUM[ever ==1 & outcome == 1]),
            sensitivity = patients_identified/patients_with_outcome,
            ppv = sum(ever ==1 & outcome == 1)/sum(ever == 1))

DT::datatable(clinician_sum_ever,
              caption = "Performance by each clinician on any event (ever)",
              options = list(scrollX = T))

```


```{r, echo=F}

a <- clinician_sum_ever %>% 
  filter(!is.na(sensitivity))


CIs <- binom::binom.confint(x=a$patients_identified, n=a$patients_with_outcome, methods="wilson")

a <- a %>% bind_cols(CIs)
a %>% 
  mutate(clinicianid = fct_reorder(as.character(clinicianid), sensitivity)) %>% 
qplot(x=clinicianid, y=patients_identified/patients_with_outcome, ymin=lower, ymax=upper, data=., ylim=c(0,1), geom = "pointrange") +
  coord_flip()


a %>% 
  ggplot(aes(sensitivity)) + geom_histogram() +
  ggtitle("Sensitivities (ever) for each clinician") +
  xlab("Sensitivity") +
  ylab("Count")


a %>% 
  group_by(role) %>% 
  summarize(number_clinicians = n_distinct(clinicianid),
            median_sens = median(sensitivity),
            q25_sens = quantile(sensitivity, .25),
            q75_sens = quantile(sensitivity, .75)) %>% 
  arrange(desc(median_sens)) %>% 
  DT::datatable(caption = "performance by professional role")


```


Below is the same plot with a courser grouping of clinicians (physician, resident, nurse). Clinicians have the highest sensitivity at 0.23 (IQR: 0.0 -.51) Different numbers!


```{r, echo =F}

a %>%
  mutate(role = case_when(
   grepl("Physician|Pyysician", role, ignore.case = T) ~ "Physician",
   grepl("Resident", role, ignore.case = T) ~ "Resident",
   grepl("Nurse", role, ignore.case = T) ~ "Nurse",
  TRUE ~ NA_character_
)) %>% 
  group_by(role) %>% 
  summarize(number_clinicians = n_distinct(clinicianid),
            median_sens = median(sensitivity),
            q25_sens = quantile(sensitivity, .25),
            q75_sens = quantile(sensitivity, .75)) %>% 
  arrange(desc(median_sens)) %>% 
  DT::datatable(caption = "performance by professional role")


```

### PPV

THere were 67 clinicians (40.11%) that never made a positive prediction.  Of the remaining 100 clinicians,  49 (49/100 = 49%) had a positive predictive value of 0. The plot below displays a histogram of positive predictive values for each clinician (median = 0.13, IQR: 0.0-0.4583). Twelve clinicians had perfect PPV. 


```{r, echo=F}

clinician_sum_ever %>%
  mutate(ppv_exists = if_else(is.na(ppv),1 , 0)) %>%
  count(ppv_exists) %>%
  mutate(ratio = n /sum(n)) %>%
  knitr::kable(caption = "Positive predictions")

clinician_sum_ever %>%
  count(ppv) %>%
  knitr::kable(caption = "Breakdown of PPV")


b <- clinician_sum_ever %>% 
  filter(!is.na(ppv))

b %>% 
  ggplot(aes(ppv)) + geom_histogram() +
  ggtitle("Positive Predictive Value for each clinician") +
  xlab("Positive Predictive Value") +
  ylab("Count")

b$ppv %>%
  quantile(na.rm = TRUE) %>%
  knitr::kable(caption = "Quantiles of PPV ever")


```



Below is a table of PPV performance by role  (physician, resident, nurse). Residens have the highest PPV at 0.33 (IQR: 0.1 -.44)


```{r, echo =F}

b <- b %>%
  mutate(role = case_when(
   grepl("Physician|Pyysician", role, ignore.case = T) ~ "Physician",
   grepl("Resident", role, ignore.case = T) ~ "Resident",
   grepl("Nurse", role, ignore.case = T) ~ "Nurse",
  TRUE ~ NA_character_
))

b %>% 
  group_by(role) %>% 
  summarize(number_clinicians = n_distinct(clinicianid),
            median_ppv = median(ppv),
            q25_ppvs = quantile(ppv, .25),
            q75_ppv = quantile(ppv, .75)) %>% 
  arrange(desc(median_ppv)) %>% 
  DT::datatable(caption = "PPV performance by professional role")


```


## Overall sensitivity between model and clincians

```{r, echo=F}

model_performance <- model %>% 
  filter(last_score > threshold & outcome == 1) %>% 
  summarize(n_distinct(ENCOUNTER_NUM))

clinician_performance <- clinician %>% 
  filter(ever == 1  & outcome == 1) %>% 
  summarize(n_distinct(ENCOUNTER_NUM))



```