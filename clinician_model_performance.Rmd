---
title: "pre-assessment performance"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document looks at the model performance of the clinician predictions with a comparison against the MARS CHARTwatch model. The CHARTwatch model usually generates predictions 3 times a day, but for a fair comparison of clinician vs model predictions, we only run the CHARTwatch at times when the clincians generated their predictions. 


```{r, echo=F, message=F, warning=F}
# load the require libraries ----------------------------------------------

# load the require libraries ----------------------------------------------
library(pROC)
library(tidyverse)

# threshold from model retraining to Aprils
threshold <- .0905

prediction_outcomes <- readr::read_csv('Z:\\LKS-CHART\\Projects\\gim_ews_preassessment_project\\data\\practitioner_predictions_outcomes.csv')
load("Z:\\LKS-CHART\\Projects\\gim_ews_preassessment_project\\data\\model_predictions.Rda")

phys_enc <- unique(prediction_outcomes$ENCOUNTER_NUM)
mod_enc <- unique(model_predictions$ENCOUNTER_NUM)

sd <- setdiff(phys_enc, mod_enc)

clinician <- prediction_outcomes %>% 
  filter(!ENCOUNTER_NUM %in% sd)

clinician <- clinician %>% 
  mutate(outcome = ifelse(icu+death+pal > 0, 1, 0))

distinct_outcomes <- prediction_outcomes %>% 
  distinct(ENCOUNTER_NUM, timestamp, OUTCOME, outcome48, OUTCOME_TS)

model <- model_predictions %>% 
  inner_join(distinct_outcomes, by = c("timestamp", "ENCOUNTER_NUM"))


clinician_sum <- clinician %>% 
  group_by(ENCOUNTER_NUM) %>% 
  summarize(mp = max(ever),
            mp48 = max(next48),
            outcome = max(outcome),
            outcome_48 = max(outcome48)) %>% 
  ungroup()

model <- model %>% 
  left_join(clinician_sum, by = c("ENCOUNTER_NUM"))




```


## Descriptive statistics

There were 169 clincians who generated 3,375 predictions on  961 patient encounters between April 30th and August 28th 2019.  The model generated 2,737 predictions for the 961 patient encounters.

Below is a breakdown of the number of predictions by professional role

```{r, echo=F}

clinician %>% 
  count(professional_role) %>% 
  ggplot(aes(professional_role, n)) +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=n), hjust=-0.1, size=3.5)+
  coord_flip() +
  theme_minimal() +
  ggtitle("Number of predictions by professional role")
```


In total there were 68 patiens (7.08%) patients who experienced an outcome at some time after a prediction was made. There were 44 patients (4.58%) that experienced an outcome within 48 hours of a prediction. Below is a table of outcomes

```{r, echo=F}

ever <- clinician %>% 
  group_by(ENCOUNTER_NUM) %>% 
  summarize(death_ever = max(death),
            icu_ever = max(icu),
            pal_ever = max(pal)) %>% 
  ungroup() %>% 
  summarize(death = sum(death_ever),
            icu = sum(icu_ever),
            pal = sum(pal_ever))

outcome48 <- clinician %>% 
  filter(outcome48 == 1) %>% 
  group_by(ENCOUNTER_NUM) %>% 
  summarize(death_ever = max(OUTCOME_TYPE == 2),
            icu_ever = max(OUTCOME_TYPE == 1),
            pal_ever = max(OUTCOME_TYPE == 3)) %>% 
  ungroup() %>% 
  summarize(death = sum(death_ever),
            icu = sum(icu_ever),
            pal = sum(pal_ever))

outcome_table <- tibble(`Outcome Type` = c("Death",
                                           "ICU",
                                           "Palliative Transfer"),
                        `Outcome Ever` = c(ever$death, ever$icu, ever$pal),
                        `Outcome in 48 hours` = c(outcome48$death, outcome48$icu,
                                                  outcome48$pal))

knitr::kable(outcome_table)

```







## Performance of All Clincians

As a baseline check of model performance, we look at the accuracy of all clinician predictions against the outcome of death/icu/palliative transfer at any point in the patient stay. We can think of each clinician as its own model. This will mean multiple clinicians can predict on the same patient at the same time. Later we will look at average performance per clinician (and by role). 


```{r, warning=F, message=F}
library(caret)

cat("AUC of outcome at any time\n")
roc_obj_ever <- roc(clinician$outcome, clinician$ever, plot=F, ci=TRUE, main = "Outcome at any time")

roc_obj_ever

cat("AUC of outcome in next 48 hours\n")
roc_obj_ever <- roc(clinician$outcome48, clinician$next48, plot=F, ci=TRUE, main = "Outcome in next 48 hours")

roc_obj_ever

confusionMatrix(factor(clinician$ever, levels = c(0, 1)), factor(clinician$outcome, levels = c(0,1)), positive = "1")


```

The AUC of all clinicians together is 0.603 (95% CI: 0.58-0.63) for an outcome occuring at any time and 0.63 (95% CI: .67-0.69) in the next 48 hours. The Sensitivity is 0.30 while the positive predictive value is .33. 


## Overall performance of the model

```{r, echo=F, message=F, warning=F}

roc_obj_ever <- roc(model$outcome, model$last_score, plot=TRUE, ci=TRUE, main = "Outcome at any time")

roc_obj_ever


roc_obj_ever <- roc(model$outcome48, model$last_score, plot=TRUE, ci=TRUE, main = "Outcome in next 48 hours")

roc_obj_ever




confusionMatrix(factor(as.numeric(model$last_score > threshold), levels = c(0, 1)), factor(model$outcome, levels = c(0,1)), positive = "1")


```





## Performance per clinician

In what follows we calculate performance metrics for each clinician and then examine those distributions to understand the average clinician performance. There are 169 clincians who provided at least 1 prediction with a median of 7 predictions per clinician (Interquartile Range: 4-16). Clincians provided predictions for a median of 7 patient encounters with an interquartile range of 4-15. 

THere were 54 clinicians (31%) who's patient never experienced an outcome. Of the remaining 115 clinicians, 63 (54.8%) did not correctly identify any outcomes (sensitivity). The plot below displays a histogram of sensitivities for each clinician (median = 0.0, IQR: 0.0 -.50). Eighteen clinicians had perfect sensitivity. 


```{r, echo=F}
clinician_sum_ever <- clinician %>% 
  group_by(clinicianid) %>% 
  summarize(role = max(professional_role),
            number_predictions = n(),
            number_positive = sum(ever),
            number_negative = sum(ever == 0),
            number_encounters = n_distinct(ENCOUNTER_NUM),
            patients_with_outcome = n_distinct(ENCOUNTER_NUM[outcome == 1]),
            correct_positive = sum(ever ==1 & outcome == 1),
            correct_negative = sum(ever == 0 & outcome == 0),
            false_negative = sum(ever == 0 & outcome == 1),
            false_positive = sum(ever == 1 & outcome == 0),
            patients_identified = n_distinct(ENCOUNTER_NUM[ever ==1 & outcome == 1]),
            sensitivity = patients_identified/patients_with_outcome,
            ppv = sum(ever ==1 & outcome == 1)/sum(ever == 1))

DT::datatable(clinician_sum_ever,
              caption = "Performance by each clinician on any event",
              options = list(scrollX = T))

```


```{r, echo=F}

a <- clinician_sum_ever %>% 
  filter(!is.na(sensitivity))


CIs <- binom.confint(x=a$patients_identified, n=a$patients_with_outcome, methods="wilson")

a <- a %>% bind_cols(CIs)
a %>% 
  mutate(clinicianid = fct_reorder(as.character(clinicianid), sensitivity)) %>% 
qplot(x=clinicianid, y=patients_identified/patients_with_outcome, ymin=lower, ymax=upper, data=., ylim=c(0,1), geom = "pointrange") +
  coord_flip()


a %>% 
  ggplot(aes(sensitivity)) + geom_histogram() +
  ggtitle("Sensitivities for each clinician") +
  xlab("Sensitivity") +
  ylab("Count")


a %>% 
  group_by(role) %>% 
  summarize(number_clinicians = n_distinct(clinicianid),
            median_sens = median(sensitivity),
            q25_sens = quantile(sensitivity, .25),
            q75_sens = quantile(sensitivity, .75)) %>% 
  arrange(desc(median_sens)) %>% 
  DT::datatable(caption = "performance by professional role")


```


Below is the same plot with a courser grouping of clinicians (physician, resident, nurse). Clinicians have the highest sensitivity at 0.23 (IQR: 0.0 -.51)


```{r, echo =F}

a <- a %>%
  mutate(role = case_when(
   grepl("Physician|Pyysician", role, ignore.case = T) ~ "Physician",
   grepl("Resident", role, ignore.case = T) ~ "Resident",
   grepl("Nurse", role, ignore.case = T) ~ "Nurse",
  TRUE ~ NA_character_
))

a %>% 
  group_by(role) %>% 
  summarize(number_clinicians = n_distinct(clinicianid),
            median_sens = median(sens),
            q25_sens = quantile(sens, .25),
            q75_sens = quantile(sens, .75)) %>% 
  arrange(desc(median_sens)) %>% 
  DT::datatable(caption = "performance by professional role")


```

### PPV

THere were 67 clinicians (39.6%) that never made a positive prediction.  Of the remaining 102 clinicians, 50 (49.0%) had a positive predictive value of 0. The plot below displays a histogram of positive predictive values for each clinician (median = 0.11, IQR: 0.0 -.50). Fourteen clinicians had perfect PPV. 


```{r, echo=F}
b <- clinician_sum %>% 
  filter(!is.na(ppv))

b %>% 
  ggplot(aes(ppv)) + geom_histogram() +
  ggtitle("Positive Predictive Value for each clinician") +
  xlab("Positive Predictive Value") +
  ylab("Count")


```



Below is a table of PPV performance by role  (physician, resident, nurse). Residens have the highest PPV at 0.33 (IQR: 0.1 -.44)


```{r, echo =F}

b <- b %>%
  mutate(role = case_when(
   grepl("Physician|Pyysician", role, ignore.case = T) ~ "Physician",
   grepl("Resident", role, ignore.case = T) ~ "Resident",
   grepl("Nurse", role, ignore.case = T) ~ "Nurse",
  TRUE ~ NA_character_
))

b %>% 
  group_by(role) %>% 
  summarize(number_clinicians = n_distinct(clinicianid),
            median_ppv = median(ppv),
            q25_ppvs = quantile(ppv, .25),
            q75_ppv = quantile(ppv, .75)) %>% 
  arrange(desc(median_ppv)) %>% 
  DT::datatable(caption = "PPV performance by professional role")


```


## Overall sensitivity between model and clincians

```{r, echo=F}

model_performance <- model %>% 
  filter(last_score > threshold & outcome == 1) %>% 
  summarize(n_distinct(ENCOUNTER_NUM))

clinician_performance <- clinician %>% 
  filter(ever == 1  & outcome == 1) %>% 
  summarize(n_distinct(ENCOUNTER_NUM))



```